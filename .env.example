# Setup-Agent (SAG) environment configuration example  
# Copy this file as .env and fill in actual values  

# ==================== Model Configuration ====================  
# Thinking model configuration (for complex reasoning)  
# Supports OpenAI o1 models and Claude Extended Thinking  
SAG_THINKING_MODEL=o4-mini
SAG_THINKING_PROVIDER=openai  
SAG_THINKING_TEMPERATURE=1 
SAG_MAX_THINKING_TOKENS=16000  
SAG_REASONING_EFFORT=medium  

# Claude Extended Thinking configuration  
SAG_CLAUDE_EXTENDED_THINKING=true  
SAG_CLAUDE_THINKING_BUDGET_TOKENS=10000  

# Action model configuration (for tool invocation)  
SAG_ACTION_MODEL=gpt-4o  
SAG_ACTION_PROVIDER=openai  
SAG_ACTION_TEMPERATURE=0.3  
SAG_MAX_ACTION_TOKENS=10000  

# ==================== API Keys ====================  
# OpenAI API key  
OPENAI_API_KEY=your_openai_api_key_here  

# Anthropic API key (required for Claude Extended Thinking)  
ANTHROPIC_API_KEY=your_anthropic_api_key_here  

# Groq API key  
# GROQ_API_KEY=your_groq_api_key_here  

# Azure API key  
# AZURE_API_KEY=your_azure_api_key_here  

# ==================== API Base URLs ====================  
# OpenAI API base URL (useful if using a proxy or custom endpoint)  
OPENAI_BASE_URL=https://api.openai.com/v1  

# Ollama base URL (if using local models)
OLLAMA_BASE_URL=http://localhost:11434

# Use ollama_chat prefix for better Ollama model responses
# SAG_USE_OLLAMA_CHAT=true  

# Azure API configuration  
# AZURE_API_BASE=your_azure_api_base_here  
# AZURE_API_VERSION=2023-12-01-preview  

# ==================== Logging Configuration ====================  
# Log level: DEBUG, INFO, WARNING, ERROR  
SAG_LOG_LEVEL=INFO  

# Log file path  
SAG_LOG_FILE=logs/sag.log  

# Enable verbose debug output (includes Claude thinking blocks)  
SAG_VERBOSE=false  

# Log file rotation size  
SAG_LOG_ROTATION=50 MB  

# Log file retention period  
SAG_LOG_RETENTION=30 days  

# ==================== Docker Configuration ====================  
# Docker base image  
SAG_DOCKER_BASE_IMAGE=ubuntu:22.04  

# Workspace path (inside the container)  
SAG_WORKSPACE_PATH=/workspace  

# ==================== Agent Configuration ====================  
# Maximum number of iterations  
SAG_MAX_ITERATIONS=50  

# Context switching threshold  
SAG_CONTEXT_SWITCH_THRESHOLD=20  

# ==================== Model Selection Notes ====================
# Thinking model selection:
# 1. Claude Extended Thinking: claude-3-5-sonnet-20241022 (recommended)
# 2. OpenAI o series: o4-mini
# 3. Ollama local models: gpt-oss:20b, llama3:70b, mixtral:8x7b
#
# Example settings:
# - Using Claude: SAG_THINKING_MODEL=claude-3-5-sonnet-20241022, SAG_THINKING_PROVIDER=anthropic
# - Using OpenAI o1: SAG_THINKING_MODEL=o4-mini, SAG_THINKING_PROVIDER=openai
# - Using Ollama: SAG_THINKING_MODEL=gpt-oss:20b, SAG_THINKING_PROVIDER=ollama

# ==================== Ollama Configuration (Local Models) ====================
# To use Ollama for fully offline operation:
#
# 1. Install Ollama from https://ollama.ai/
# 2. Pull desired models:
#    ollama pull gpt-oss:20b     # For thinking (20B params)
#    ollama pull llama3:8b       # For actions (8B params, faster)
#
# Example Ollama configuration:
# SAG_THINKING_MODEL=gpt-oss:20b
# SAG_THINKING_PROVIDER=ollama
# SAG_ACTION_MODEL=llama3:8b
# SAG_ACTION_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434
# SAG_USE_OLLAMA_CHAT=true        # Recommended for better responses
#
# Recommended Ollama models:
# Thinking: gpt-oss:20b, llama3:70b, mixtral:8x7b, qwen2:72b
# Action: llama3:8b, mistral:7b, codellama:13b, qwen2:7b
#
# Performance tips:
# - Ollama models use prompt-based tool calling (no native function calling)
# - Use lower temperatures: 0.1-0.3 for actions, 0.7-1.0 for thinking
# - Consider GPU acceleration for larger models
# - Monitor memory usage with 70B+ parameter models  

# ==================== Claude Thinking Configuration ====================  
# Claude thinking budget tokens (1024, 2048, 4096)  
SAG_THINKING_BUDGET_TOKENS=10000  